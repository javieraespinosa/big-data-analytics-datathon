{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1OkFCDYoNgW"
   },
   "source": [
    "# Exploration of WebArchives: Demo (Docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lpX3RxISYwY"
   },
   "source": [
    "## Spark init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIVXv8GEquwZ"
   },
   "source": [
    "Initialize spark in [single-node cluster](https://docs.databricks.com/clusters/single-node.html) and configure pyspark with the AUT toolkit & GraphFrame libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gb-ED0zhZyGt"
   },
   "outputs": [],
   "source": [
    "%run ../scripts/spark-init-docker.ipynb\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndMg7cPUWMOy"
   },
   "source": [
    "## Creating Web Archives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLKHVGdBrVei"
   },
   "source": [
    "Example of [web archiving using WGET](https://wiki.archiveteam.org/index.php/Wget_with_WARC_output) as web crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCD8rDmBW80O"
   },
   "outputs": [],
   "source": [
    "%%writefile input.txt\n",
    "http://www.espinosa-oviedo.com\n",
    "http://www.vargas-solar.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes (see [WGET manual](https://www.gnu.org/software/wget/manual/wget.html)):\n",
    "\n",
    "* Add `--recursive` for crawling the totality of a website. \n",
    "* Remove `--no-warc-compression` for generating compressed WARC files.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSCGG4i7WLzc"
   },
   "outputs": [],
   "source": [
    "LEVEL=1       # maximum number of links to follow (i.e, crawl depth)\n",
    "WAIT=0.1      # num. seconds to wait between consecutive calls\n",
    "\n",
    "IN_FILE       = \"input.txt\"  # list of URLs to crawl\n",
    "OUT_DIR       = \"WARC\"       # folder where crawl results will be stored\n",
    "OUT_WARC_FILE = \"out\"        # prefix for WARC files\n",
    "OUT_LOG_FILE  = \"log.txt\"    # file containing WGET log\n",
    "\n",
    "# https://www.gnu.org/software/wget/manual/wget.html\n",
    "!wget \\\n",
    "  --delete-after -nd \\\n",
    "  --input-file={IN_FILE}  \\\n",
    "  --level={LEVEL}    \\\n",
    "  --no-parent        \\\n",
    "  --wait={WAIT}      \\\n",
    "  --random-wait      \\\n",
    "  --adjust-extension \\\n",
    "  --reject=css,js,xml,rss,php  \\\n",
    "  --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15\" \\\n",
    "  --warc-file=out    \\\n",
    "  --warc-max-size=300m  \\\n",
    "  --no-warc-keep-log    \\\n",
    "  --no-warc-compression \\\n",
    "  --output-file={OUT_LOG_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TXBnJDcXtvG"
   },
   "outputs": [],
   "source": [
    "# Move resulting files to the OUT_DIR folder\n",
    "!mkdir -p {OUT_DIR} \n",
    "!mv *.warc*  *.txt  {OUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOC7tF2wQVEk"
   },
   "source": [
    "## Querying Web Archives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI5i2nTYxbb6"
   },
   "source": [
    "Note: \n",
    "\n",
    "* **AUT generates dataframes**. See the [AUT dataframe schemas](https://aut.docs.archivesunleashed.org/docs/dataframe-schemas) and the [Spark SQL guide](https://spark.apache.org/docs/3.0.0/sql-getting-started.html) for more info.\n",
    "* More examples are available in the [AUT documentation](https://aut.docs.archivesunleashed.org/docs/home). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4qxI_jrYETD"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from aut import *\n",
    "\n",
    "WARCs_path = \"WARC/*.warc*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfb7i8qvxZOO"
   },
   "source": [
    "### Extract webpages URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "durenU1UDgaK"
   },
   "source": [
    "Spark Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AApBC8RLGzcb"
   },
   "outputs": [],
   "source": [
    "WebArchive(sc, sqlContext, WARCs_path) \\\n",
    "  .webpages() \\\n",
    "  .select(\"url\") \\\n",
    "  .show(20, False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuJGuReC-8MK"
   },
   "source": [
    "Spark SQL equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHqQfKw-9po3"
   },
   "outputs": [],
   "source": [
    "df = WebArchive(sc, sqlContext, WARCs_path).webpages()\n",
    "df.createOrReplaceTempView(\"webpages\")\n",
    "\n",
    "sql='''\n",
    "    SELECT url \n",
    "    FROM webpages \n",
    "'''\n",
    "\n",
    "sqlContext.sql(sql).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLfl84iK0jdG"
   },
   "source": [
    "### Extract Top-Level Domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHnHv-mFBqQ8"
   },
   "source": [
    "Uses a [User Defined Function](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html) (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtQSGzj-A06z"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LeAz-6zGBOq3"
   },
   "outputs": [],
   "source": [
    "import tldextract\n",
    "tldextract.extract('http://forums.news.cnn.com/')    # See https://github.com/john-kurkowski/tldextract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojRT-AI3Dnix"
   },
   "source": [
    "Spark Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGA2HW010mNr"
   },
   "outputs": [],
   "source": [
    "import tldextract\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "@udf(\"string\")\n",
    "def extract_tld(s):\n",
    "    return tldextract.extract(s).suffix\n",
    "\n",
    "WebArchive(sc, sqlContext, WARCs_path) \\\n",
    "  .webpages() \\\n",
    "  .select(extract_tld(\"url\").alias(\"tld\")) \\\n",
    "  .groupBy(\"tld\") \\\n",
    "  .count() \\\n",
    "  .sort(desc(\"count\"))\\\n",
    "  .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7P6ctd2z_nUt"
   },
   "source": [
    "Spark SQL equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIa09KKn_isx"
   },
   "outputs": [],
   "source": [
    "df = WebArchive(sc, sqlContext, WARCs_path).webpages()\n",
    "df.createOrReplaceTempView(\"webpages\")\n",
    "\n",
    "sqlContext.udf.register(\"extract_tld\", extract_tld)\n",
    "\n",
    "sql='''\n",
    "    SELECT tld, count(tld) AS count\n",
    "    FROM (\n",
    "      SELECT extract_tld(url) AS tld \n",
    "      FROM webpages \n",
    "    )\n",
    "    GROUP BY tld\n",
    "    ORDER BY count DESC\n",
    "'''\n",
    "\n",
    "sqlContext.sql(sql).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGwXPm_BhTqW"
   },
   "source": [
    "### Count words in web pages\n",
    "\n",
    "Uses a [User Defined Function](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html) (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QapsdBfWiNPA"
   },
   "outputs": [],
   "source": [
    "from aut import remove_html, remove_http_header\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "@udf(\"Integer\")\n",
    "def word_count(s):\n",
    "  return len( s.split() )\n",
    "\n",
    "WebArchive(sc, sqlContext, WARCs_path) \\\n",
    "  .webpages()\\\n",
    "  .withColumn(\"text\", remove_html( remove_http_header(\"content\") ))\\\n",
    "  .withColumn(\"word_count\", word_count(\"text\"))\\\n",
    "  .select(\"text\", \"word_count\")\\\n",
    "  .show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb0XcphID2Es"
   },
   "source": [
    "Spark SQL equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNzSOL7CD4cB"
   },
   "outputs": [],
   "source": [
    "df = WebArchive(sc, sqlContext, WARCs_path)\\\n",
    "        .webpages()\\\n",
    "        .withColumn(\"text\", remove_html( remove_http_header(\"content\") ))   # AUT's remove_html & remove_http_header work only with dataframes\n",
    "\n",
    "df.createOrReplaceTempView(\"webpages_text\")\n",
    "\n",
    "@udf(\"Integer\")\n",
    "def word_count(s):\n",
    "  return len( s.split() )\n",
    "\n",
    "sqlContext.udf.register(\"word_count\", word_count)\n",
    "\n",
    "sql='''\n",
    "    SELECT text, word_count(text) AS word_count \n",
    "    FROM   webpages_text \n",
    "'''\n",
    "\n",
    "sqlContext.sql(sql).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZpHqIW12k9J"
   },
   "source": [
    "### Count links between domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LysFUZvk2n5e"
   },
   "outputs": [],
   "source": [
    "from aut import extract_domain\n",
    "\n",
    "edges = WebArchive(sc, sqlContext, WARCs_path) \\\n",
    "  .webgraph()\\\n",
    "  .withColumn(\"src_domain\",  extract_domain(\"src\"))  \\\n",
    "  .withColumn(\"dest_domain\", extract_domain(\"dest\")) \\\n",
    "  .select([\"src_domain\", \"dest_domain\"])\\\n",
    "  .groupBy([\"src_domain\", \"dest_domain\"])\\\n",
    "  .count()\n",
    "\n",
    "edges.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUBWxl34GBv8"
   },
   "source": [
    "Plots using [NetworkX](http://networkx.org) and [matplotlib](http:/:matplotlib.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnRVGKY05ps9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "df = edges.limit(10).toPandas()\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df, \n",
    "    source=\"src_domain\", \n",
    "    target=\"dest_domain\", \n",
    "    edge_key=\"dest_domain\", \n",
    "    edge_attr=\"count\"\n",
    ")\n",
    "\n",
    "pos = nx.planar_layout(G)\n",
    "options = {\n",
    "    \"node_size\": 1000,\n",
    "    \"node_color\": \"#bc5090\",\n",
    "    \"node_shape\": \"o\",\n",
    "    \"alpha\": 0.5,\n",
    "    \"linewidths\": 4,\n",
    "    \"font_size\": 10,\n",
    "    \"font_color\": \"black\",\n",
    "    \"width\": 2,\n",
    "    \"edge_color\": \"grey\",\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "nx.draw(G, pos, with_labels=True, **options)\n",
    "labels = {e: G.edges[e][\"count\"] for e in G.edges}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_85Mpyfzlgt"
   },
   "source": [
    "### Distribution of HTTP Status Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHxYFefq92Tq"
   },
   "outputs": [],
   "source": [
    "codes = WebArchive(sc, sqlContext, WARCs_path) \\\n",
    "  .all()\\\n",
    "  .groupBy('http_status_code')\\\n",
    "  .count()\n",
    "  \n",
    "codes.show(20, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtYem_rCG3P9"
   },
   "source": [
    "Plots using [Plotly Express](https://plotly.com/python/plotly-express/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDGwQqAp79Ts"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(\n",
    "    codes.toPandas(),\n",
    "    x='http_status_code', \n",
    "    y='count'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnGWkIvRjd8M"
   },
   "source": [
    "### Export graph to Gephi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRsQ7c-WwSBq"
   },
   "source": [
    "See [Gephi Graph Viz Platform](http://gephi.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwMaZdQnj3X_"
   },
   "outputs": [],
   "source": [
    "graph = WebArchive(sc, sqlContext, WARCs_path) \\\n",
    "          .webgraph() \\\n",
    "          .groupBy(\"crawl_date\", remove_prefix_www(extract_domain(\"src\")).alias(\"src_domain\"), remove_prefix_www(extract_domain(\"dest\")).alias(\"dest_domain\")) \\\n",
    "          .count() \\\n",
    "          .filter((col(\"dest_domain\").isNotNull()) & (col(\"dest_domain\") !=\"\")) \\\n",
    "          .filter((col(\"src_domain\").isNotNull()) & (col(\"src_domain\") !=\"\")) \\\n",
    "          .orderBy(desc(\"count\")) \\\n",
    "          .collect()\n",
    "\n",
    "WriteGEXF(graph, \"links-for-gephi.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCrPMFwzTOmW"
   },
   "source": [
    "### Store results on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypVqt2DpUppM"
   },
   "source": [
    "Save as `csv` file with header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEd3-cUITzbS"
   },
   "outputs": [],
   "source": [
    "WebArchive(sc, sqlContext, WARCs_path) \\\n",
    "  .webgraph()\\\n",
    "  .limit(10)\\\n",
    "  .write.format('csv').save(\"webgraph\", header='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOCKywq5U9qe"
   },
   "source": [
    "Save as `parquet` file (header automatically included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaHZe7AxVDuU"
   },
   "outputs": [],
   "source": [
    "WebArchive(sc, sqlContext, WARCs_path) \\\n",
    "  .webgraph()\\\n",
    "  .limit(10)\\\n",
    "  .write.parquet(\"webgraph.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U25drGB0VXoQ"
   },
   "source": [
    "Read csv/parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywF_O0N8VbpX"
   },
   "outputs": [],
   "source": [
    "# load parquet files\n",
    "df = sqlContext.read.parquet(\"webgraph.parquet\")\n",
    "df.show(2)\n",
    "df.printSchema()\n",
    "\n",
    "# load csv files\n",
    "df = sqlContext.read.option(\"header\", True).csv(\"webgraph\")\n",
    "df.show(2)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYVHEPHkysOR"
   },
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmGQc7fEWObC"
   },
   "source": [
    "### Collecting LIFRANUM Web Archives from **Google Storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AB0vmL5JbzJB"
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "DIR=\"LIFRANUM\"\n",
    "!mkdir -p $DIR\n",
    "\n",
    "!gsutil -m cp -r gs://cpe-lyon/LIFRANUM/repo-ecritures-num $DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obsAx21wboYM"
   },
   "source": [
    "### Accelerating operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0Rbjch0b29x"
   },
   "source": [
    "Caching dataframes in RAM accelerates spark operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXlXxJV4bpON"
   },
   "outputs": [],
   "source": [
    "WARCs_path = \"LIFRANUM/repo-ecritures-num/out-00000.warc.gz\"\n",
    "\n",
    "webpages = WebArchive(sc, sqlContext, WARCs_path).webpages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIKlQgZXcjHl"
   },
   "source": [
    "Without cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVD19YPbcDid"
   },
   "outputs": [],
   "source": [
    "webpages.count()    # slow: spark loads the data, filter webpages, compute new columns, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoO1ZIzgcpvV"
   },
   "source": [
    "Using cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqhwmQY5c0Zi"
   },
   "outputs": [],
   "source": [
    "webpages.cache().count()    # first time slow because all previous operations are re executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXjsJ3a8c6Nl"
   },
   "outputs": [],
   "source": [
    "webpages.count()            # second time is faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzT5ETtqTiQo"
   },
   "source": [
    "### Loading image from bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Knp3FfEBTkPw"
   },
   "source": [
    "[Reading image from string base64](https://dev.to/bl4ckst0n3/image-processing-how-to-read-image-from-string-in-python-pf8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roSpmc_UUuI6"
   },
   "outputs": [],
   "source": [
    "# Get an image's bytes from a WARC file\n",
    "res = WebArchive(sc, sqlContext, WARCs_path) \\\n",
    "  .images() \\\n",
    "  .select(\"bytes\")\\\n",
    "  .take(1)\n",
    "\n",
    "img_base64_string = res[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQiLofzKUNUZ"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "# load image from bytes\n",
    "decoded_string = io.BytesIO( base64.b64decode(img_base64_string) )\n",
    "Image.open(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0u6_--fNvS4"
   },
   "source": [
    "### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVx-zhVKOrVe"
   },
   "source": [
    "Uses [Spacy.io](https://spacy.io/usage/linguistic-features#named-entities-101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jwyyx7baOzzd"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Recognizes english NERs\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KyyTf5UPWzc"
   },
   "source": [
    "Entity Name Recognition using a webpage in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HmQGQH0PJso"
   },
   "outputs": [],
   "source": [
    "WebArchive(sc, sqlContext, WARCs_path) \\\n",
    "  .webpages() \\\n",
    "  .select(\"*\", remove_html(remove_http_header(\"content\")).alias(\"text\"))\\\n",
    "  .createOrReplaceTempView(\"webpages\")\n",
    "\n",
    "sql='''\n",
    "    SELECT language, text\n",
    "    FROM   webpages\n",
    "    WHERE  language=='en' AND text <> ''\n",
    "'''\n",
    "\n",
    "res = sqlContext.sql(sql).take(1)   # returns a list of 1 element\n",
    "txt = res[0][1]                     # \"text\" attribute from first element\n",
    "\n",
    "doc = nlp(txt)\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk5RPJcifjF_"
   },
   "source": [
    "## LIFRANUM WARC files (backup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIFRANUM's WARC files are stored in google drive too. Uncomment the lines below if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !pip install -q gdown\n",
    "\n",
    "#!gdown https://drive.google.com/drive/folders/1xqDsY5KOeK5OMhW39EH37l79Pn-v59B_?usp=sharing -O ./LIFRANUM/autre --folder\n",
    "#!gdown https://drive.google.com/drive/folders/170j3r23YJBlOpGsKrcZRSs3bqrS03qhi?usp=sharing -O ./LIFRANUM/cartoweb --folder\n",
    "#!gdown https://drive.google.com/drive/folders/1NLuWLOldfmpwPeAr9Th_HCeH6ZoSw0zr?usp=sharing -O ./LIFRANUM/lifranum-method --folder\n",
    "#!gdown https://drive.google.com/drive/folders/1wehg3nnCks9iVIvuXMZ5u685ocq__dQe?usp=sharing -O ./LIFRANUM/repo-ecritures-num --folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4lpX3RxISYwY",
    "ndMg7cPUWMOy",
    "mOC7tF2wQVEk",
    "zfb7i8qvxZOO",
    "wLfl84iK0jdG",
    "cGwXPm_BhTqW",
    "7ZpHqIW12k9J",
    "n_85Mpyfzlgt",
    "LnGWkIvRjd8M",
    "oCrPMFwzTOmW",
    "rmGQc7fEWObC",
    "obsAx21wboYM",
    "OzT5ETtqTiQo",
    "E0u6_--fNvS4"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
